from backend.api_gateway.libs.milkyhoop_prisma import Prisma
import asyncio
import signal
import logging
import os
import json
import grpc
import hashlib
import re
from grpc import aio
from grpc_health.v1 import health, health_pb2, health_pb2_grpc
from app.config import settings
from app import tenant_parser_pb2_grpc as pb_grpc
from app import tenant_parser_pb2 as pb
from app.services.llm_parser import parse_intent_entities
from openai import OpenAI

# RAG CRUD integration
try:
    from app import ragcrud_service_pb2 as rag_pb
    from app import ragcrud_service_pb2_grpc as rag_pb_grpc
    RAG_CRUD_AVAILABLE = True
except ImportError:
    RAG_CRUD_AVAILABLE = False

# Level 13 Complete Integration
try:
    from app import cust_context_pb2 as context_pb
    from app import cust_context_pb2_grpc as context_pb_grpc
    LEVEL13_AVAILABLE = True
except ImportError:
    LEVEL13_AVAILABLE = False

try:
    from app import cust_reference_pb2 as ref_pb
    from app import cust_reference_pb2_grpc as ref_pb_grpc
    REFERENCE_AVAILABLE = True
except ImportError:
    REFERENCE_AVAILABLE = False

print(f"‚úÖ Services - RAG: {RAG_CRUD_AVAILABLE}, Level 13: {LEVEL13_AVAILABLE}, Reference: {REFERENCE_AVAILABLE}")

prisma = Prisma()
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(name)s: %(message)s")
logger = logging.getLogger(__name__)

# ========================================
# üéØ UNIFIED CONFIDENCE ENGINE - CTO APPROVED
# ========================================

class UnifiedConfidenceEngine:
    """
    Universal Confidence Engine for Cost Optimization
    Target: 98.7% cost reduction (Rp 150 ‚Üí Rp 1.9 per query)
    """
    
    def __init__(self):
        self.daily_cost_tracker = {}  # Circuit breaker protection
        self.cache = {}  # Query caching (CTO enhancement)        # Clear any old cached out-of-scope queries
        self.cache.clear()
        logger.info("üîÑ Cache cleared for scope filtering deployment")
        
        # Initialize OpenAI client if available
        try:
            self.openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        except:
            self.openai_client = None
            logger.warning("‚ö†Ô∏è OpenAI client not available - LLM synthesis disabled")
    

    def pre_llm_scope_filter(self, query: str, tenant_context: dict) -> dict:
        """First line of defense - instant rejection for obvious out-of-scope"""
        
        query_lower = query.lower().strip()
        
        # Universal blacklist (non-business topics)
        universal_blacklist = [
            'cuaca', 'weather', 'politik', 'political', 'olahraga', 'sport',
            'resep', 'recipe', 'agama', 'religion', 'gosip', 'celebrity',
            'berita', 'news', 'film', 'movie', 'musik', 'music', 'game'
        ]
        
        if any(word in query_lower for word in universal_blacklist):
            return {
                "should_deflect": True,
                "reason": "universal_non_business",
                "confidence": 0.05
            }
        
        # Extract tenant business relevance from ACTUAL FAQ data
        tenant_keywords = tenant_context.get('faq_keywords', [])
        business_indicators = ['harga', 'biaya', 'beli', 'order', 'booking', 'jam', 'lokasi', 'cara', 'layanan', 'produk']
        
        # Check relevance against tenant's ACTUAL business keywords
        tenant_matches = sum(1 for keyword in tenant_keywords if keyword in query_lower)
        business_matches = sum(1 for word in business_indicators if word in query_lower)
        
        if tenant_matches > 0 or business_matches > 0:
            return {"should_deflect": False, "reason": "business_relevant"}
        
        # If no business relevance found, likely out of scope
        return {"should_deflect": True, "reason": "no_business_relevance", "confidence": 0.15}

    async def get_tenant_context_dynamic(self, tenant_id: str) -> dict:
        """DYNAMIC: Extract tenant context from actual FAQ data"""
        
        try:
            # Get tenant's actual FAQ data via RAG CRUD
            if hasattr(self, '_rag_stub') and self._rag_stub:
                rag_request = rag_pb.SearchFAQRequest()
                rag_request.tenant_id = tenant_id
                rag_request.query = "overview"  # Get general FAQs
                rag_request.limit = 50  # Get more FAQs for analysis
                
                rag_response = await self._rag_stub.SearchFAQ(rag_request, timeout=3.0)
                
                if rag_response.status == "success" and rag_response.faqs:
                    return self.extract_business_context_from_faqs(rag_response.faqs, tenant_id)
            
            # Fallback: basic context
            return self.get_basic_business_context(tenant_id)
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Could not get dynamic context for {tenant_id}: {e}")
            return self.get_basic_business_context(tenant_id)

    def extract_business_context_from_faqs(self, faqs, tenant_id: str) -> dict:
        """Extract business type and keywords from actual FAQ content"""
        
        all_faq_text = " ".join([faq.content.lower() for faq in faqs[:20]])  # Analyze first 20 FAQs
        
        # Auto-detect business type from FAQ patterns
        business_type = self.detect_business_type_from_content(all_faq_text)
        
        # Extract business-specific keywords from FAQ content
        faq_keywords = self.extract_keywords_from_faqs(faqs)
        
        # Generate business topics based on detected type
        business_topics = self.generate_business_topics(business_type, faq_keywords)
        
        return {
            'tenant_id': tenant_id,
            'business_type': business_type,
            'business_topics': business_topics,
            'faq_keywords': faq_keywords,
            'context_source': 'dynamic_faq_analysis'
        }

    def detect_business_type_from_content(self, faq_text: str) -> str:
        """Auto-detect business type from FAQ content patterns"""
        
        # Banking indicators
        banking_keywords = ['rekening', 'tabungan', 'kredit', 'bank', 'transfer', 'setoran', 'saldo']
        banking_score = sum(1 for word in banking_keywords if word in faq_text)
        
        # Restaurant indicators  
        restaurant_keywords = ['menu', 'makanan', 'minuman', 'cafe', 'restaurant', 'pesan', 'delivery']
        restaurant_score = sum(1 for word in restaurant_keywords if word in faq_text)
        
        # Healthcare indicators
        health_keywords = ['dokter', 'konsultasi', 'medical', 'kesehatan', 'klinik', 'rumah sakit']
        health_score = sum(1 for word in health_keywords if word in faq_text)
        
        # E-commerce indicators
        ecommerce_keywords = ['produk', 'toko', 'belanja', 'checkout', 'keranjang', 'pengiriman']
        ecommerce_score = sum(1 for word in ecommerce_keywords if word in faq_text)
        
        # Return type with highest score
        scores = {
            'layanan perbankan': banking_score,
            'restoran/cafe': restaurant_score, 
            'layanan kesehatan': health_score,
            'toko online': ecommerce_score
        }
        
        detected_type = max(scores, key=scores.get)
        
        # If no clear pattern, use generic
        if scores[detected_type] < 2:
            return 'layanan bisnis'
            
        return detected_type

    def extract_keywords_from_faqs(self, faqs) -> list:
        """Extract meaningful keywords from FAQ content"""
        
        import re
        from collections import Counter
        
        # Combine all FAQ content
        all_text = " ".join([faq.content.lower() for faq in faqs])
        
        # Extract meaningful words (3+ characters, not common words)
        words = re.findall(r'\b[a-zA-Z]{3,}\b', all_text)
        
        # Filter out common words
        stopwords = ['dan', 'atau', 'untuk', 'dengan', 'yang', 'adalah', 'akan', 'dapat', 'bisa', 'ada', 'tidak', 'kami', 'anda', 'saya']
        meaningful_words = [word for word in words if word not in stopwords]
        
        # Get most frequent keywords
        word_counts = Counter(meaningful_words)
        top_keywords = [word for word, count in word_counts.most_common(20) if count >= 2]
        
        return top_keywords

    def generate_business_topics(self, business_type: str, keywords: list) -> list:
        """Generate business topics based on type and keywords"""
        
        # Base topics for any business
        base_topics = ['layanan', 'produk', 'harga', 'jam operasional', 'lokasi', 'kontak']
        
        # Add type-specific topics
        type_specific = {
            'layanan perbankan': ['tabungan', 'kredit', 'transfer', 'kartu debit', 'internet banking'],
            'restoran/cafe': ['menu', 'makanan', 'minuman', 'reservasi', 'delivery'],
            'layanan kesehatan': ['konsultasi', 'dokter', 'jadwal', 'treatment', 'medical checkup'],
            'toko online': ['katalog produk', 'pemesanan', 'pembayaran', 'pengiriman', 'return']
        }
        
        specific_topics = type_specific.get(business_type, [])
        
        # Combine base + specific + keywords (limit to prevent prompt bloat)
        all_topics = base_topics + specific_topics + keywords[:10]
        
        return list(set(all_topics))[:15]  # Limit to 15 topics

    def get_basic_business_context(self, tenant_id: str) -> dict:
        """Fallback: basic business context when dynamic extraction fails"""
        
        return {
            'tenant_id': tenant_id,
            'business_type': 'layanan bisnis',
            'business_topics': ['layanan', 'produk', 'harga', 'jam operasional', 'kontak'],
            'faq_keywords': ['layanan', 'produk', 'harga', 'beli', 'order'],
            'context_source': 'fallback_basic'
        }

    def calculate_universal_confidence(self, query: str, faq_results: list = None, tenant_id: str = "default") -> float:
        """Enhanced FAQ relevance scoring with high confidence boost"""
        
        query_lower = query.lower().strip()
        
        # üõ°Ô∏è PRE-LLM SCOPE FILTERING - Universal non-business topics
        non_business_keywords = [
            'cuaca', 'weather', 'politik', 'political', 'olahraga', 'sport',
            'resep', 'recipe', 'agama', 'religion', 'berita', 'news', 
            'film', 'movie', 'musik', 'music', 'game', 'gosip'
        ]
        
        if any(word in query_lower for word in non_business_keywords):
            logger.info(f"üö´ Out-of-scope query detected: {query[:50]}...")
            return 0.05  # Force polite deflection
        
        confidence = 0.0
        
        # üöÄ ENHANCED FAQ MATCHING - Higher confidence for exact matches
        if faq_results and len(faq_results) > 0:
            best_faq = faq_results[0]
            base_score = getattr(best_faq, 'similarity_score', 0.0)
            confidence = base_score
            
            # üéØ EXACT QUESTION MATCH BOOST (+0.40)
            if hasattr(best_faq, 'question'):
                faq_question_lower = best_faq.question.lower()
                # Check for very high similarity in question structure
                query_words = set(query_lower.split())
                faq_words = set(faq_question_lower.split())
                word_overlap = len(query_words & faq_words) / max(len(query_words), 1)
                
                if word_overlap >= 0.7:  # 70% word overlap
                    confidence += 0.40
                    logger.info(f"üéØ High FAQ question similarity detected: {word_overlap:.2f}")
            
            # üîç EXACT PHRASE MATCHING (+0.35)
            if hasattr(best_faq, 'content'):
                faq_content_lower = best_faq.content.lower()
                # Check for exact phrase matches
                key_phrases = ['setoran awal', 'tahapan xpresi', 'biaya admin', 'cara buka rekening']
                phrase_matches = sum(1 for phrase in key_phrases if phrase in query_lower and phrase in faq_content_lower)
                
                if phrase_matches > 0:
                    confidence += phrase_matches * 0.15
                    logger.info(f"üîç Exact phrase matches found: {phrase_matches}")
            
            # Keyword overlap boost (existing logic but enhanced)
            query_keywords = self.extract_keywords(query_lower)
            faq_keywords = self.extract_keywords(best_faq.content.lower())
            keyword_overlap = len(set(query_keywords) & set(faq_keywords))
            confidence += min(keyword_overlap * 0.08, 0.20)  # Increased from 0.05 to 0.08
        
        # Universal question patterns (+0.30) - reduced from 0.40 to balance
        question_patterns = ['berapa', 'apa', 'bagaimana', 'gimana', 'kapan', 'dimana', 'siapa', 'cara']
        if any(pattern in query_lower for pattern in question_patterns):
            confidence += 0.30
        
        # Universal business topics (+0.25) - slightly reduced
        business_topics = ['harga', 'biaya', 'tarif', 'setoran', 'admin', 'jam', 'buka', 'lokasi', 'alamat']
        topic_matches = sum(1 for word in business_topics if word in query_lower)
        confidence += min(topic_matches * 0.08, 0.25)
        
        # Specific intent patterns
        if self.is_pricing_query(query_lower):
            confidence += 0.15
        elif self.is_process_query(query_lower):
            confidence += 0.12
        
        final_confidence = min(confidence, 1.0)
        logger.info(f"üéØ Final confidence calculated: {final_confidence:.3f} for query: {query[:30]}...")
        
        return final_confidence
    
    def enhanced_decision_engine(self, confidence: float) -> dict:
        """Cost-optimized decision tree"""
        
        if confidence >= 0.75:
            return {
                "route": "direct_faq_only",
                "model": None,
                "tokens_input": 0,
                "tokens_output": 0,
                "cost_per_query": 0.0,  # Rp 0
                "faq_count": 1,
                "context_level": "none"
            }
        elif confidence >= 0.40:
            return {
                "route": "gpt_3.5_synthesis", 
                "model": "gpt-3.5-turbo",
                "tokens_input": 300,
                "tokens_output": 150,
                "cost_per_query": 7.0,  # Rp 7.0
                "faq_count": 2,
                "context_level": "medium"
            }
        elif confidence >= 0.20:
            return {
                "route": "gpt_3.5_deep_analysis",
                "model": "gpt-3.5-turbo",
                "tokens_input": 600,
                "tokens_output": 200,
                "cost_per_query": 12.4,  # Rp 12.4
                "faq_count": 3,
                "context_level": "full"
            }
        else:
            return {
                "route": "polite_deflection",
                "model": None,
                "tokens_input": 0,
                "tokens_output": 0,
                "cost_per_query": 0.0,  # Rp 0
                "faq_count": 0,
                "context_level": "none"
            }
    
    def extract_keywords(self, text: str) -> list:
        """Extract meaningful keywords"""
        stop_words = {'yang', 'adalah', 'dan', 'atau', 'untuk', 'dengan', 'dari', 'ke', 'di', 'pada', 'ini', 'itu'}
        words = re.findall(r'\b\w+\b', text.lower())
        return [word for word in words if word not in stop_words and len(word) > 2]
    
    def is_pricing_query(self, text: str) -> bool:
        """Detect pricing queries"""
        return any(keyword in text for keyword in ['berapa', 'harga', 'biaya', 'tarif', 'setoran', 'admin', 'mahal', 'murah'])
    
    def is_process_query(self, text: str) -> bool:
        """Detect process queries"""
        return any(keyword in text for keyword in ['cara', 'bagaimana', 'gimana', 'langkah', 'proses'])
    
    def is_location_query(self, text: str) -> bool:
        """Detect location queries"""
        return any(keyword in text for keyword in ['dimana', 'lokasi', 'alamat', 'cabang', 'kantor'])
    
    def extract_answer_only(self, faq_content: str) -> str:
        """Extract clean answer from FAQ"""
        if faq_content.startswith("Q:") and "\nA:" in faq_content:
            return faq_content.split("\nA:", 1)[1].strip()
        return faq_content.strip()
    

    def should_cache_query(self, query: str) -> bool:
        """Determine if query should be cached based on scope"""
        query_lower = query.lower().strip()
        
        # Never cache out-of-scope queries
        non_business_keywords = [
            'cuaca', 'weather', 'politik', 'political', 'olahraga', 'sport',
            'resep', 'recipe', 'agama', 'religion', 'berita', 'news', 
            'film', 'movie', 'musik', 'music', 'game', 'gosip'
        ]
        
        if any(word in query_lower for word in non_business_keywords):
            return False  # Don't cache out-of-scope queries
            
        # Only cache business-relevant queries
        return True

        def get_cache_key(self, tenant_id: str, query: str) -> str:
        """Generate cache key (CTO enhancement)"""
        return hashlib.md5(f"{tenant_id}:{query.lower()}".encode()).hexdigest()
    
    def check_circuit_breaker(self, tenant_id: str) -> bool:
        """Circuit breaker - daily budget protection"""
        daily_budget = 50000  # Rp 50,000 daily limit
        current_cost = self.daily_cost_tracker.get(tenant_id, 0)
        
        if current_cost > daily_budget:
            logger.warning(f"üö® Circuit breaker: {tenant_id} exceeded daily budget")
            return True
        return False
    
    def track_cost(self, tenant_id: str, cost: float):
        """Track daily costs"""
        if tenant_id not in self.daily_cost_tracker:
            self.daily_cost_tracker[tenant_id] = 0
        self.daily_cost_tracker[tenant_id] += cost
    
    async def call_gpt_3_5(self, prompt: str, max_tokens: int = 150) -> str:
        """Call GPT-3.5 with token budget enforcement"""
        if not self.openai_client:
            raise Exception("OpenAI client not available")
            
        try:
            response = await asyncio.get_event_loop().run_in_executor(
                None,
                lambda: self.openai_client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"‚ùå OpenAI API error: {e}")
            raise

class TenantParserServicer(pb_grpc.IntentParserServiceServicer):
    def __init__(self):
        self.rag_crud_target = "ragcrud_service:5001"
        self.context_target = "cust_context:5008"
        self.reference_target = "cust_reference:5013"
        
        # Initialize Unified Confidence Engine
        self.confidence_engine = UnifiedConfidenceEngine()
        
        # OPTIMIZATION 1: Reuse gRPC Channels - Connection Pooling
        self._context_channel = None
        self._rag_crud_channel = None
        self._reference_channel = None
        self._context_stub = None
        self._rag_crud_stub = None
        self._reference_stub = None
        
        # Initialize channels
        asyncio.create_task(self._initialize_channels())
        
    async def _initialize_channels(self):
        """Initialize and cache gRPC channels for reuse"""
        try:
            if LEVEL13_AVAILABLE:
                self._context_channel = aio.insecure_channel(self.context_target)
                self._context_stub = context_pb_grpc.CustContextServiceStub(self._context_channel)
                logger.info("‚úÖ Context channel initialized")
                
            if RAG_CRUD_AVAILABLE:
                self._rag_crud_channel = aio.insecure_channel(self.rag_crud_target)
                self._rag_crud_stub = rag_pb_grpc.RagCrudServiceStub(self._rag_crud_channel)
                logger.info("‚úÖ RAG CRUD channel initialized")
                
            if REFERENCE_AVAILABLE:
                self._reference_channel = aio.insecure_channel(self.reference_target)
                self._reference_stub = ref_pb_grpc.Cust_referenceStub(self._reference_channel)
                logger.info("‚úÖ Reference channel initialized")
                
        except Exception as e:
            logger.error(f"‚ùå Channel initialization error: {e}")

    async def _cleanup_channels(self):
        """Cleanup gRPC channels on shutdown"""
        try:
            if self._context_channel:
                await self._context_channel.close()
            if self._rag_crud_channel:
                await self._rag_crud_channel.close()
            if self._reference_channel:
                await self._reference_channel.close()
            logger.info("‚úÖ All channels cleaned up")
        except Exception as e:
            logger.error(f"‚ùå Channel cleanup error: {e}")
    
    def sanitize_protobuf_for_json(self, data: dict) -> dict:
        """Convert protobuf objects to JSON-serializable Python types
        
        This method handles the conversion of protobuf-specific types like
        RepeatedScalarContainer to standard Python types that can be serialized to JSON.
        """
        if not isinstance(data, dict):
            return data
            
        sanitized = {}
        for key, value in data.items():
            if value is None:
                sanitized[key] = None
            elif isinstance(value, (str, int, float, bool)):
                # Primitive types are already JSON-serializable
                sanitized[key] = value
            elif isinstance(value, dict):
                # Recursively sanitize nested dictionaries
                sanitized[key] = self.sanitize_protobuf_for_json(value)
            elif hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
                # Convert any iterable (like RepeatedScalarContainer) to list
                try:
                    sanitized[key] = list(value)
                except:
                    sanitized[key] = str(value)
            else:
                # For any other type, convert to string
                sanitized[key] = str(value)
                
        return sanitized
        
    def extract_tenant_from_context(self, grpc_context) -> str:
        """OPTIMIZATION 4: Extract tenant_id from gRPC context metadata"""
        try:
            metadata = dict(grpc_context.invocation_metadata())
            return metadata.get('tenant-id', 'default')
        except:
            return 'default'

    async def parse_intent_stage(self, message: str, tenant_id: str) -> dict:
        """Pipeline Stage 1: Parse customer intent"""
        try:
            result = parse_intent_entities(message)
            logger.info(f"üìù [{tenant_id}] Intent: {result.get('intent')}")
            return result
        except Exception as e:
            logger.error(f"‚ùå [{tenant_id}] Intent parsing error: {e}")
            return {"intent": "general_inquiry", "entities": {}}
    
    def analyze_message_for_intelligence(self, message: str, tenant_id: str) -> dict:
        """
        üß† Level 13 Message Analysis - Extract mood, intent, and lead scoring from message text
        """
        message_lower = message.lower()
        
        # 1. MOOD DETECTION (Indonesian keywords)
        mood = "neutral"
        mood_confidence = 0.5
        
        # Happy/Excited indicators
        if any(word in message_lower for word in ["excited", "senang", "suka", "bagus", "mantap", "keren", "wow", "hebat", "top"]):
            mood = "happy"
            mood_confidence = 0.8
        
        # Frustrated indicators  
        elif any(word in message_lower for word in ["kesel", "marah", "lama", "down", "!!!", "banget!!", "tutup rekening", "bete", "kesal"]):
            mood = "frustrated" 
            mood_confidence = 0.9
            
        # Urgent/Serious indicators
        elif any(word in message_lower for word in ["sekarang juga", "urgent", "cepat", "segera", "serius", "penting", "langsung"]):
            mood = "urgent"
            mood_confidence = 0.8
            
        # Curious indicators
        elif any(word in message_lower for word in ["gimana", "bagaimana", "apa", "kenapa", "mengapa", "bisa", "penasaran"]):
            mood = "curious"
            mood_confidence = 0.7
        
        # 2. LEAD SCORING ANALYSIS
        lead_score = 0.0
        buying_signals = []
        
        # High-value customer indicators
        if any(indicator in message_lower for indicator in ["ceo", "pengusaha", "startup", "direktur", "owner", "pemilik"]):
            lead_score += 0.3
            buying_signals.append("high_status")
            
        # Revenue/money indicators
        if any(indicator in message_lower for indicator in ["omzet", "revenue", "pendapatan", "juta", "miliar", "m per tahun", "500m"]):
            lead_score += 0.4
            buying_signals.append("high_revenue")
            
        # Immediate need indicators
        if any(indicator in message_lower for indicator in ["sekarang", "mau buka", "butuh", "perlu", "ingin", "pengen"]):
            lead_score += 0.2
            buying_signals.append("immediate_need")
            
        # Premium product interest
        if any(indicator in message_lower for indicator in ["private banking", "premium", "platinum", "tapres", "prioritas", "eksklusif"]):
            lead_score += 0.3
            buying_signals.append("premium_interest")
            
        # Investment/business indicators
        if any(indicator in message_lower for indicator in ["invest", "investasi", "bisnis", "usaha", "modal", "dana"]):
            lead_score += 0.2
            buying_signals.append("investment_minded")
        
        # 3. INTENT CLASSIFICATION
        intent = "general_inquiry"
        
        if any(word in message_lower for word in ["buka rekening", "daftar", "apply", "mendaftar", "registrasi"]):
            intent = "account_opening"
        elif any(word in message_lower for word in ["harga", "biaya", "admin", "bunga", "tarif", "berapa"]):
            intent = "pricing_inquiry"  
        elif any(word in message_lower for word in ["syarat", "dokumen", "proses", "persyaratan", "cara"]):
            intent = "requirement_inquiry"
        elif any(word in message_lower for word in ["bahan", "produk", "jenis", "apa", "macam", "tipe"]):
            intent = "product_inquiry"
        elif any(word in message_lower for word in ["keluhan", "komplain", "masalah", "error", "gagal"]):
            intent = "complaint"
        
        # 4. FRUSTRATION LEVEL
        frustration_level = 0.0
        if mood == "frustrated":
            frustration_level = 0.8
            if "!!!" in message or "banget!!" in message_lower:
                frustration_level = 0.9
        
        # 5. PROACTIVE SUGGESTIONS
        proactive_suggestions = []
        if intent == "account_opening" and lead_score > 0.5:
            proactive_suggestions.append("Mau saya hubungkan dengan relationship manager untuk layanan prioritas?")
        elif mood == "frustrated":
            proactive_suggestions.append("Mari saya bantu dengan solusi yang lebih cepat")
        elif intent == "pricing_inquiry":
            proactive_suggestions.append("Saya bisa jelaskan paket yang paling sesuai budget Anda")
        
        return {
            "mood": mood,
            "mood_confidence": mood_confidence,
            "lead_score": min(lead_score, 1.0),  # Cap at 1.0
            "buying_signals": buying_signals,
            "intent": intent,
            "frustration_level": frustration_level,
            "proactive_suggestions": proactive_suggestions,
            "analysis_source": "local_nlp",
            "tenant_id": tenant_id
        }

    async def enrich_with_level13_intelligence(self, session_id: str, tenant_id: str, message: str) -> dict:
        """Pipeline Stage 2: Level 13 Intelligence with LOCAL MESSAGE ANALYSIS"""
        try:
            # üß† PERFORM LOCAL MESSAGE ANALYSIS (No gRPC needed!)
            local_analysis = self.analyze_message_for_intelligence(message, tenant_id)
            
            logger.info(f"üß† [{tenant_id}] Message Analysis: mood={local_analysis['mood']}, lead_score={local_analysis['lead_score']:.2f}, signals={local_analysis['buying_signals']}")
            
            # Enhanced intelligence data ready for response enhancement
            intelligence_data = {
                "mood": local_analysis["mood"],
                "mood_confidence": local_analysis["mood_confidence"],
                "lead_score": local_analysis["lead_score"],
                "buying_signals": local_analysis["buying_signals"],
                "intent": local_analysis["intent"],
                "intent_stage": "information_gathering" if local_analysis["lead_score"] < 0.7 else "purchase_decision",
                "frustration_level": local_analysis["frustration_level"],
                "proactive_suggestions": local_analysis["proactive_suggestions"],
                "recommended_tone": "empathetic" if local_analysis["mood"] == "frustrated" else "professional_friendly",
                "tone_guidelines": f"Use {local_analysis['mood']} tone with {local_analysis['mood_confidence']:.1f} confidence",
                "analysis_complete": True,
                "source": "local_nlp_analysis"
            }
            
            logger.info(f"üöÄ [{tenant_id}] Level 13 LOCAL intelligence complete - {len(intelligence_data)} signals")
            return intelligence_data
            
        except Exception as e:
            logger.error(f"‚ùå [{tenant_id}] Level 13 local analysis failed: {e}")
            return {
                "mood": "neutral",
                "lead_score": 0.0,
                "buying_signals": [],
                "intent": "general_inquiry",
                "analysis_complete": False,
                "error": str(e)
            }

    async def fetch_content_stage(self, tenant_id: str, message: str, resolved_message: str) -> dict:
        """Pipeline Stage 3: Enhanced FAQ content with Confidence Engine"""        # 1. Smart caching - only cache business queries
        cache_key = self.confidence_engine.get_cache_key(tenant_id, resolved_message)
        
        # Check if this query should be cached
        if self.confidence_engine.should_cache_query(resolved_message):
            if cache_key in self.confidence_engine.cache:
                logger.info(f"üíæ [{tenant_id}] Business query cache hit!")
                return self.confidence_engine.cache[cache_key]
        else:
            logger.info(f"üö´ [{tenant_id}] Out-of-scope query - bypassing cache")
        
        # 2. Check circuit breaker
        if self.confidence_engine.check_circuit_breaker(tenant_id):
            return {
                "response": f"Maaf, layanan untuk {tenant_id} sedang dibatasi sementara.",
                "route_taken": "circuit_breaker",
                "cost_estimate": 0.0,
                "confidence": 0.0
            }
        
        # 3. Fetch FAQ documents
        faq_results = []
        if RAG_CRUD_AVAILABLE and self._rag_crud_stub:
            try:
                request = rag_pb.FuzzySearchRequest()
                request.tenant_id = tenant_id
                request.search_content = resolved_message
                request.similarity_threshold = 0.7
                
                response = await self._rag_crud_stub.FuzzySearchDocuments(request, timeout=10.0)
                
                if response.documents:
                    faq_results = list(response.documents)
                    logger.info(f"‚úÖ [{tenant_id}] Found {len(faq_results)} FAQ matches")
                else:
                    logger.info(f"‚ÑπÔ∏è [{tenant_id}] No FAQ matches found")
                    
            except Exception as e:
                logger.error(f"‚ùå [{tenant_id}] FAQ fetch error: {e}")
        
        # 4. Calculate universal confidence
        confidence = self.confidence_engine.calculate_universal_confidence(resolved_message, faq_results)
        logger.info(f"üìä [{tenant_id}] Confidence: {confidence:.3f} for: {resolved_message[:40]}...")
        
        # 5. Decision routing
        decision = self.confidence_engine.enhanced_decision_engine(confidence)
        
        # 6. Route execution
        if decision["route"] == "direct_faq_only":
            # Zero-cost direct response
            if faq_results:
                response = self.confidence_engine.extract_answer_only(faq_results[0].content)
            else:
                response = f"Informasi untuk {tenant_id} sedang tidak tersedia saat ini."
            cost_estimate = 0.0
            
        elif decision["route"] == "polite_deflection":
            # Polite deflection for very low confidence
            response = f"Maaf, saya tidak memiliki informasi yang tepat untuk pertanyaan tersebut. Silakan hubungi tim {tenant_id} langsung untuk bantuan lebih lanjut."
            cost_estimate = 0.0
            
        elif decision["route"] in ["gpt_3.5_synthesis", "gpt_3.5_deep_analysis"]:
            # LLM synthesis required
            try:
                if decision["context_level"] == "medium":
                    prompt = self.build_medium_prompt(resolved_message, faq_results[:2])
                else:  # full context
                    prompt = self.build_deep_prompt(resolved_message, faq_results[:3])
                
                response = await self.confidence_engine.call_gpt_3_5(
                    prompt, 
                    max_tokens=decision["tokens_output"]
                )
                cost_estimate = decision["cost_per_query"]
                
            except Exception as e:
                logger.error(f"‚ùå [{tenant_id}] LLM synthesis failed: {e}")
                # Fallback to direct FAQ
                if faq_results:
                    response = self.confidence_engine.extract_answer_only(faq_results[0].content)
                else:
                    response = f"Maaf ada kendala teknis untuk {tenant_id}."
                cost_estimate = 0.0
        else:
            # Fallback
            response = f"Maaf ada kendala untuk {tenant_id}."
            cost_estimate = 0.0
        
        # 7. Track costs
        self.confidence_engine.track_cost(tenant_id, cost_estimate)
        
        # 8. Prepare result
        result = {
            "response": response,
            "confidence": confidence,
            "route_taken": decision["route"],
            "cost_estimate": cost_estimate,
            "tokens_used": decision.get("tokens_input", 0) + decision.get("tokens_output", 0),
            "metadata": {
                "faqs_used": len(faq_results[:decision["faq_count"]]),
                "llm_called": decision["model"] is not None,
                "daily_cost": self.confidence_engine.daily_cost_tracker.get(tenant_id, 0)
            }
        }        # 9. Smart cache storage - only cache business queries
        if self.confidence_engine.should_cache_query(resolved_message):
            self.confidence_engine.cache[cache_key] = result
            logger.info(f"üíæ [{tenant_id}] Business query cached")
        else:
            logger.info(f"üö´ [{tenant_id}] Out-of-scope query - not cached")
        
        logger.info(f"üéØ [{tenant_id}] Route: {decision['route']}, Cost: Rp {cost_estimate}, Confidence: {confidence:.3f}")
        
        return result
    
def build_medium_prompt(self, query: str, faq_context: list, tenant_context: dict = None) -> str:
        """Enhanced prompt with dynamic business scope constraints"""
        if tenant_context is None:
            tenant_context = self.get_basic_business_context("default")
            
        business_type = tenant_context.get('business_type', 'layanan bisnis')
        business_topics = tenant_context.get('business_topics', [])
        context_text = "
".join([f"FAQ: {faq.content}" for faq in faq_context])
        
        return f"""CRITICAL: You are customer service for {business_type} ONLY.

ALLOWED TOPICS: {', '.join(business_topics)}

FOR ANY NON-{business_type.upper()} QUESTION, respond EXACTLY:
"Maaf, saya hanya membantu pertanyaan tentang {business_type}. Ada yang bisa saya bantu tentang layanan kami?"

NEVER answer: weather, politics, sports, recipes, other businesses not related to {business_type}.

FAQ Context: {context_text}
Customer Question: {query}

Remember: ONLY {business_type} topics allowed."""
    
    def build_deep_prompt(self, query: str, faq_context: list, tenant_context: dict = None) -> str:
        """Enhanced deep analysis prompt with dynamic scope constraints"""
        if tenant_context is None:
            tenant_context = self.get_basic_business_context("default")
            
        business_type = tenant_context.get('business_type', 'layanan bisnis')
        context_text = "
".join([f"FAQ {i+1}: {faq.content}" for i, faq in enumerate(faq_context)])
        
        return f"""CRITICAL: You are an expert customer service representative for {business_type} ONLY.

STRICT RULE: Only discuss {business_type} topics. For anything else, politely redirect to {business_type} services.

Context: {context_text}
Customer Question: {query}

Provide detailed, helpful response about {business_type} ONLY."""


    async def resolve_references_stage(self, session_id: str, tenant_id: str, message: str) -> str:
        """Pipeline Stage 4: Resolve Indonesian references"""
        if not REFERENCE_AVAILABLE or not self._reference_stub:
            return message
            
        try:
            request = ref_pb.ReferenceRequest()
            request.session_id = session_id
            request.reference_text = message
            request.tenant_id = tenant_id
            request.context_query = message
            
            response = await self._reference_stub.ResolveReference(request, timeout=5.0)
            
            if hasattr(response, 'resolved_message') and response.resolved_message:
                logger.info(f"‚úÖ [{tenant_id}] Reference resolved")
                return response.resolved_message
            else:
                return message
                
        except Exception as e:
            logger.error(f"‚ùå [{tenant_id}] Reference resolution error: {e}")
            return message

    def enhance_response_stage(self, response: str, intelligence_data: dict, tenant_id: str) -> str:
        """Pipeline Stage 5: Apply Level 13 intelligence to enhance response"""
        if not intelligence_data:
            return response
            
        enhanced_response = response
        
        # Mood-based enhancement
        mood = intelligence_data.get('mood', 'neutral')
        if mood == 'happy':
            if not any(enhanced_response.lower().startswith(prefix) for prefix in ['wah', 'senang', 'bagus']):
                enhanced_response = f"Senang bisa membantu! {enhanced_response} üòä"
        elif mood == 'frustrated':
            enhanced_response = f"Maaf jika ada kendala sebelumnya. {enhanced_response} Semoga ini membantu! üôè"
        elif mood == 'curious':
            enhanced_response = f"Pertanyaan yang bagus! {enhanced_response}"
        elif mood == 'urgent':
            enhanced_response = f"Saya akan bantu dengan cepat. {enhanced_response}"
        
        # Intent stage enhancement
        intent_stage = intelligence_data.get('intent_stage', '')
        if intent_stage == 'purchase_decision':
            enhanced_response += "\n\nApakah Anda ingin melanjutkan ke tahap berikutnya? Saya siap membantu! üöÄ"
        elif intent_stage == 'information_gathering':
            enhanced_response += "\n\nAda informasi lain yang ingin Anda ketahui? üòä"
        
        # Lead scoring enhancement
        lead_score = intelligence_data.get('lead_score', 0)
        if lead_score > 0.7:
            enhanced_response += "\n\nTerlihat Anda cukup serius dengan ini. Mau saya hubungkan dengan tim specialist kami? üìû"
        
        # Frustration recovery
        frustration_level = intelligence_data.get('frustration_level', 0)
        if frustration_level > 0.5:
            recovery_suggestions = intelligence_data.get('recovery_suggestions', [])
            if recovery_suggestions:
                enhanced_response += f"\n\nüí° Saran: {recovery_suggestions[0] if recovery_suggestions else 'Mari kita coba pendekatan yang lebih sederhana.'}"
        
        # Proactive suggestions
        proactive_suggestions = intelligence_data.get('proactive_suggestions', [])
        if proactive_suggestions:
            enhanced_response += f"\n\nüí° Mungkin Anda juga tertarik: {proactive_suggestions[0]}"
        
        return enhanced_response

    async def DoSomething(self, request, context):
        """Enhanced Pipeline with Unified Confidence Engine"""
        session_id = request.user_id if hasattr(request, 'user_id') else "default"
        message = request.input if hasattr(request, 'input') else ""
        tenant_id = self.extract_tenant_from_context(context)
        
        logger.info(f"üéØ [{tenant_id}] Enhanced pipeline start: {message[:50]}...")
        
        try:
            # Pipeline Stage 1: Parse Intent
            intent_result = await self.parse_intent_stage(message, tenant_id)
            
            # Pipeline Stage 2: Level 13 Intelligence
            intelligence_data = await self.enrich_with_level13_intelligence(session_id, tenant_id, message)
            
            # Pipeline Stage 3: Resolve References
            resolved_message = await self.resolve_references_stage(session_id, tenant_id, message)
            
            # Pipeline Stage 4: Enhanced FAQ Content with Confidence Engine
            content_result = await self.fetch_content_stage(tenant_id, message, resolved_message)
            
            # Pipeline Stage 5: Enhance Response
            enhanced_response = self.enhance_response_stage(
                content_result["response"], 
                intelligence_data, 
                tenant_id
            )
            
            # CRITICAL FIX: Sanitize protobuf data before JSON serialization
            sanitized_intelligence = self.sanitize_protobuf_for_json(intelligence_data)
            
            # Final Response with Confidence Metadata
            result = {
                "tenant_id": tenant_id,
                "intent": intent_result.get("intent", "general_inquiry"),
                "entities": intent_result.get("entities", {}),
                "response": enhanced_response,
                "reference_resolved": resolved_message != message,
                "level13_intelligence": sanitized_intelligence,
                "mood": sanitized_intelligence.get('mood', 'neutral'),
                "lead_score": sanitized_intelligence.get('lead_score', 0),
                "recommended_tone": sanitized_intelligence.get('recommended_tone', 'professional_friendly'),
                "confidence_metadata": {
                    "confidence_score": content_result.get("confidence", 0.0),
                    "route_taken": content_result.get("route_taken", "unknown"),
                    "cost_estimate": content_result.get("cost_estimate", 0.0),
                    "tokens_used": content_result.get("tokens_used", 0),
                    "optimization_active": True
                }
            }
            
            logger.info(f"üöÄ [{tenant_id}] Enhanced pipeline complete - Route: {content_result.get('route_taken')}, Cost: Rp {content_result.get('cost_estimate', 0)}")
            
            return pb.IntentParserResponse(
                status="success",
                result=json.dumps(result, ensure_ascii=False)
            )
            
        except Exception as e:
            logger.error(f"üî• [{tenant_id}] Pipeline error: {e}", exc_info=True)
            return pb.IntentParserResponse(
                status="error",
                result=json.dumps({
                    "tenant_id": tenant_id,
                    "intent": "general_inquiry",
                    "entities": {},
                    "response": f"Maaf ada kendala teknis untuk {tenant_id}, silakan coba lagi.",
                    "level13_intelligence": {},
                    "confidence_metadata": {"error": str(e)}
                }, ensure_ascii=False)
            )

    async def HealthCheck(self, request, context):
        return request

async def serve() -> None:
    listen_addr = f"[::]:{settings.GRPC_PORT}"
    server = aio.server()
    
    servicer = TenantParserServicer()
    pb_grpc.add_IntentParserServiceServicer_to_server(servicer, server)
    
    health_servicer = health.HealthServicer()
    health_pb2_grpc.add_HealthServicer_to_server(health_servicer, server)
    health_servicer.set("tenant_parser.IntentParserService", health_pb2.HealthCheckResponse.SERVING)
    
    server.add_insecure_port(listen_addr)
    
    logger.info("üöÄ Enhanced Confidence Engine Tenant Parser listening on port %s", settings.GRPC_PORT)
    logger.info("üéØ Cost Optimization: 98.7% reduction target (Rp 150 ‚Üí Rp 1.9)")
    logger.info("‚ö° Features: Universal confidence, caching, circuit breaker, LLM synthesis")
    logger.info(f"üß† Level 13 Intelligence: {'‚úÖ Available' if LEVEL13_AVAILABLE else '‚ùå Unavailable'}")
    logger.info(f"üìö RAG CRUD: {'‚úÖ Available' if RAG_CRUD_AVAILABLE else '‚ùå Unavailable'}")
    logger.info("üåê Universal: Works with ANY business type")
    
    await server.start()

    def handle_shutdown(*_):
        logger.info("üõë Shutting down Enhanced Confidence Engine...")
        asyncio.create_task(servicer._cleanup_channels())
        asyncio.create_task(server.stop(grace=10.0))

    for sig in (signal.SIGTERM, signal.SIGINT):
        signal.signal(sig, handle_shutdown)

    await server.wait_for_termination()

if __name__ == "__main__":
    asyncio.run(serve())